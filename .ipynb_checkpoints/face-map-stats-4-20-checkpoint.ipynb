{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0f60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ParentImport\n",
    "import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipyplot\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.pyplot import imshow\n",
    "# from trainer import Trainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acc55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE 6248\n",
      "TEXT SIZE 695\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       9a22372d22a52397.mp4\n",
       "1       9bc4f1306bb8e2cd.mp4\n",
       "2       88fe2a902a9d8cc7.mp4\n",
       "3       1f3cbda142d0944a.mp4\n",
       "4       63bed62257daccaf.mp4\n",
       "                ...         \n",
       "6938    f5c4bfb20068f129.mp4\n",
       "6939    b7b7b7f6db7f6e2e.mp4\n",
       "6940    c3fc75289ae3b41e.mp4\n",
       "6941    519e97d4c90aaa1c.mp4\n",
       "6942    38cd7b295a5cfda5.mp4\n",
       "Name: filename, Length: 6943, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset(basedir='datasets')\n",
    "print(dataset.is_fake('9a22372d22a52397.mp4'))\n",
    "dataset.all_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdbc1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'9a22372d22a52397.mp4': [2, 1.0], '9bc4f1306bb8e2cd.mp4': [1, 1.0], '88fe2a902a9d8cc7.mp4': [2, 1.0], '1f3cbda142d0944a.mp4': [1, 1.0], '63bed62257daccaf.mp4': [1, 1.0], 'ee8e8d4a59a95d5f.mp4': [1, 1.0], '04011d0f6efa6d85.mp4': [1, 1.0], 'cbcf9320b4e4d9f9.mp4': [1, 1.0], '262a25215787616d.mp4': [1, 1.0], 'fbca2df503111454.mp4': [1, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "face_map_stats = json.loads(open('./face_map_stats-4-20.json').read())\n",
    "print(len(face_map_stats))\n",
    "print(face_map_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552cf702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aaa52aca0a4644b9a2048b8e76925f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FU 10 100.0\n"
     ]
    }
   ],
   "source": [
    "statuses = []\n",
    "face_metrics = []\n",
    "vid_labels = []\n",
    "names = list(face_map_stats.keys())\n",
    "\n",
    "for vid_name in tqdm(names):\n",
    "    vid_stats = face_map_stats[vid_name]\n",
    "    has_face = vid_stats[1]\n",
    "    face_metrics.append(has_face)\n",
    "    is_fake = dataset.is_fake(vid_name)\n",
    "    vid_labels.append(is_fake)\n",
    "    \n",
    "    if is_fake:\n",
    "        if has_face == 0:\n",
    "            # is fake, face not detected\n",
    "            statuses.append('F+')\n",
    "        else:\n",
    "            # is fake, face detected\n",
    "            statuses.append('FU')\n",
    "    else:\n",
    "        if has_face == 0:\n",
    "            # not fake, no face detected\n",
    "            statuses.append('R-')\n",
    "        else:\n",
    "            # not fake, face detected\n",
    "            statuses.append('RU')\n",
    "    \n",
    "for status in set(statuses):\n",
    "    count = statuses.count(status)\n",
    "    percentage = round(100 * count / len(statuses), 2)\n",
    "    print(status, count, percentage)\n",
    "    \n",
    "statuses = np.array(statuses)\n",
    "face_metrics = np.array(face_metrics)\n",
    "vid_labels = np.array(vid_labels)\n",
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd10376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "failed_real_videos = names[np.where(statuses == 'R-')]\n",
    "failed_fake_videos = names[np.where(statuses == 'F+')]\n",
    "print(len(failed_real_videos), failed_real_videos)\n",
    "print(len(failed_fake_videos), failed_fake_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e993cf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nevery_n_frames = 20\\nvidobj = dataset.get_face_locations(\\n    '86fb36004cbb9227.mp4', rescale=1,\\n    every_n_frames=every_n_frames\\n)\\n\\nprint(vidobj.video_faces)\\nvideo_faces = vidobj.video_faces\\nframe_index = list(video_faces.keys())[0]\\nloc1 = video_faces[frame_index][0]\\nimage = vidobj.out_video[frame_index // every_n_frames]\\ntop, right, bottom, left = loc1\\ncrop_image = image[top:bottom, left:right]\\n    \\n%matplotlib inline\\nimshow(crop_image)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "every_n_frames = 20\n",
    "vidobj = dataset.get_face_locations(\n",
    "    '86fb36004cbb9227.mp4', rescale=1,\n",
    "    every_n_frames=every_n_frames\n",
    ")\n",
    "\n",
    "print(vidobj.video_faces)\n",
    "video_faces = vidobj.video_faces\n",
    "frame_index = list(video_faces.keys())[0]\n",
    "loc1 = video_faces[frame_index][0]\n",
    "image = vidobj.out_video[frame_index // every_n_frames]\n",
    "top, right, bottom, left = loc1\n",
    "crop_image = image[top:bottom, left:right]\n",
    "    \n",
    "%matplotlib inline\n",
    "imshow(crop_image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3212c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 ['86fb36004cbb9227.mp4', 'fcba83aed209eb27.mp4', 'c10d8ed5abfd1eab.mp4', '60e38a133f7c619c.mp4', 'e416b3215ebf823c.mp4', '7cefa46323893b8c.mp4', 'e7c1248f1566506d.mp4', '13f9093ff81a3fb7.mp4', '23b70adaff750216.mp4', 'b7c0cbb0fa0dd1eb.mp4', '19d81e9aad1dab6b.mp4', 'b52113b4702e6fbe.mp4', '80a8c02a5a99f8f9.mp4', '637add700674c976.mp4', '3d9643987cdc139c.mp4', '4d1c4b1e7f86f6bf.mp4', '318b4e0b4db37132.mp4', '231cd47c1ed53535.mp4', 'c6e5d2e44c4d9e19.mp4', 'f970753c9348d6d9.mp4', '652c274b7e5f098d.mp4', '466120c9ca99b722.mp4', 'd27c1c217aae3e70.mp4', '18be5a88d3b7fb4e.mp4', '2dadd883c06ea06f.mp4', 'ee4895d55e2cbb67.mp4', 'b2c90c071c0a924a.mp4', '85e6445fdaa8eff9.mp4']\n",
      "86fb36004cbb9227.mp4 faces 0\n",
      "fcba83aed209eb27.mp4 faces 0\n",
      "c10d8ed5abfd1eab.mp4 faces 0\n",
      "60e38a133f7c619c.mp4 faces 0\n",
      "e416b3215ebf823c.mp4 faces 1\n",
      "7cefa46323893b8c.mp4 faces 0\n",
      "e7c1248f1566506d.mp4 faces 1\n",
      "13f9093ff81a3fb7.mp4 faces 2\n",
      "23b70adaff750216.mp4 faces 12\n",
      "b7c0cbb0fa0dd1eb.mp4 faces 15\n",
      "19d81e9aad1dab6b.mp4 faces 1\n",
      "b52113b4702e6fbe.mp4 faces 0\n",
      "80a8c02a5a99f8f9.mp4 faces 0\n",
      "637add700674c976.mp4 faces 1\n",
      "3d9643987cdc139c.mp4 faces 0\n",
      "4d1c4b1e7f86f6bf.mp4 faces 0\n",
      "318b4e0b4db37132.mp4 faces 0\n"
     ]
    }
   ],
   "source": [
    "every_n_frames = 20\n",
    "all_failed_videos = \"\"\"\n",
    "86fb36004cbb9227.mp4\n",
    "fcba83aed209eb27.mp4\n",
    "c10d8ed5abfd1eab.mp4\n",
    "60e38a133f7c619c.mp4\n",
    "e416b3215ebf823c.mp4\n",
    "7cefa46323893b8c.mp4\n",
    "e7c1248f1566506d.mp4\n",
    "13f9093ff81a3fb7.mp4\n",
    "23b70adaff750216.mp4\n",
    "b7c0cbb0fa0dd1eb.mp4\n",
    "19d81e9aad1dab6b.mp4\n",
    "b52113b4702e6fbe.mp4\n",
    "80a8c02a5a99f8f9.mp4\n",
    "637add700674c976.mp4\n",
    "3d9643987cdc139c.mp4\n",
    "4d1c4b1e7f86f6bf.mp4\n",
    "318b4e0b4db37132.mp4\n",
    "231cd47c1ed53535.mp4\n",
    "c6e5d2e44c4d9e19.mp4\n",
    "f970753c9348d6d9.mp4\n",
    "652c274b7e5f098d.mp4\n",
    "466120c9ca99b722.mp4\n",
    "d27c1c217aae3e70.mp4\n",
    "18be5a88d3b7fb4e.mp4\n",
    "2dadd883c06ea06f.mp4\n",
    "ee4895d55e2cbb67.mp4\n",
    "b2c90c071c0a924a.mp4\n",
    "85e6445fdaa8eff9.mp4\n",
    "\"\"\".strip().split()\n",
    "\n",
    "sucessful = 0\n",
    "failed = 0\n",
    "print(len(all_failed_videos), all_failed_videos)\n",
    "\n",
    "for filename in all_failed_videos:\n",
    "    vidobj = dataset.get_face_locations(\n",
    "        filename, rescale=0.5, every_n_frames=every_n_frames\n",
    "    )\n",
    "    \n",
    "    num_faces_detected = len(vidobj.video_faces)\n",
    "    print(f'{filename} faces {num_faces_detected}')\n",
    "    \n",
    "    if num_faces_detected > 0:\n",
    "        sucessful += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "    \n",
    "print(f'SUCESSFUL {sucessful} FAILED {failed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cf59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "86fb36004cbb9227.mp4 faces 1\n",
    "fcba83aed209eb27.mp4 faces 1\n",
    "c10d8ed5abfd1eab.mp4 faces 1\n",
    "60e38a133f7c619c.mp4 faces 2\n",
    "e416b3215ebf823c.mp4 faces 4\n",
    "7cefa46323893b8c.mp4 faces 0\n",
    "e7c1248f1566506d.mp4 faces 0\n",
    "13f9093ff81a3fb7.mp4 faces 5\n",
    "23b70adaff750216.mp4 faces 16\n",
    "b7c0cbb0fa0dd1eb.mp4 faces 16\n",
    "19d81e9aad1dab6b.mp4 faces 2\n",
    "b52113b4702e6fbe.mp4 faces 0\n",
    "80a8c02a5a99f8f9.mp4 faces 1\n",
    "637add700674c976.mp4 faces 2\n",
    "3d9643987cdc139c.mp4 faces 0\n",
    "4d1c4b1e7f86f6bf.mp4 faces 0\n",
    "318b4e0b4db37132.mp4 faces 0\n",
    "231cd47c1ed53535.mp4 faces 0\n",
    "c6e5d2e44c4d9e19.mp4 faces 0\n",
    "f970753c9348d6d9.mp4 faces 0\n",
    "652c274b7e5f098d.mp4 faces 0\n",
    "466120c9ca99b722.mp4 faces 0\n",
    "d27c1c217aae3e70.mp4 faces 0\n",
    "18be5a88d3b7fb4e.mp4 faces 0\n",
    "2dadd883c06ea06f.mp4 faces 15\n",
    "ee4895d55e2cbb67.mp4 faces 16\n",
    "b2c90c071c0a924a.mp4 faces 28\n",
    "85e6445fdaa8eff9.mp4 faces 6\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a85d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ultrasharp videos\n",
    "dabac5f072e1226f\n",
    "\n",
    "blur videos\n",
    "681fc525a206f128 \n",
    "3e8ebb54d3ea5f5e\n",
    "\n",
    "horizontal distorted videos\n",
    "183e7aa54ff16150\n",
    "e7c1248f1566506d\n",
    "b57a96e8a6eb5db7\n",
    "\n",
    "vertical distorted videos\n",
    "4d1c4b1e7f86f6bf\n",
    "\n",
    "normal videos\n",
    "d30576149a1c89f2\n",
    "\n",
    "square pixel blurred videos\n",
    "3f6f42b96c3b0252 \n",
    "6b7b072580418f47\n",
    "4614973dd3795b6a\n",
    "\"\"\"\n",
    "# failed_real_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_n_frames = 40\n",
    "vidobj = dataset.get_face_locations(\n",
    "    '4d1c4b1e7f86f6bf.mp4', rescale=1,\n",
    "    every_n_frames=every_n_frames\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "video_faces = vidobj.video_faces\n",
    "\n",
    "if len(video_faces) != 0:\n",
    "    # fig.add_subplot(2, 2, 1)\n",
    "    frame_index = list(video_faces.keys())[5]\n",
    "    loc1 = video_faces[frame_index][0]\n",
    "    image = vidobj.out_video[frame_index // every_n_frames]\n",
    "\n",
    "    top, right, bottom, left = loc1\n",
    "    # print(image)\n",
    "    crop_image = image[top:bottom, left:right]\n",
    "    print(crop_image.shape)\n",
    "    # plt.figure()\n",
    "    # %matplotlib inline\n",
    "    %matplotlib inline\n",
    "    imshow(crop_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b531794",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "print(a[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(video_faces)\n",
    "# face_images = vidobj.get_face_images(video_faces, every_n_frames)\n",
    "ipyplot.plot_images(vidobj.out_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_frames = vidobj.auto_resize()\n",
    "print(resized_frames)\n",
    "dataset.get_face_locations(\n",
    "    resized_frames, rescale=resized_frames.rescale,\n",
    "    every_n_frames=resized_frames.frames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipyplot.plot_images(resized_frames.out_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f636ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(video_faces)\n",
    "face_images = resized_frames.get_face_images(video_faces, every_n_frames)\n",
    "print(face_images)\n",
    "ipyplot.plot_images(face_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156df37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1dd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
